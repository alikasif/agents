{"testRunData": {"testFile": ".\\evals\\test_semantic_rag.py", "testCases": [{"name": "test_case", "input": "what is llm based routing?", "actualOutput": "LLM-based routing refers to a technique where a Large Language Model (LLM) is used directly to analyze an input (for example, a user query)             and determine what should happen next\u2014such as which workflow, tool, or sub-agent to use. The LLM is given a prompt that asks it to interpret the input and                 output a specific identifier or instruction that represents the next step. This process occurs at inference time, meaning the LLM itself is actively                     involved in making routing decisions as the system runs, rather than relying on predefined rules or a separately trained classification model.", "expectedOutput": "LLM based routing uses LLM model to analyze the input and output a specific identifier or instruction that indicates the next step or destination.             The agentic system then reads this output and directs the workflow accordingly", "retrievalContext": ["LLM based routing uses LLM model to decide the next step."], "success": true, "metricsData": [{"name": "Correctness [GEval]", "threshold": 0.5, "success": true, "score": 0.85, "reason": "The actual output closely matches the expected output, providing all required elements: it explains that an LLM analyzes input and outputs an identifier or instruction for the next step. It adds extra detail about inference time and contrasts with rule-based/classification approaches, which is not required but does not detract. The only minor shortcoming is that it does not explicitly mention the agentic system reading the output and directing the workflow, though this is implied. Overall, the requirements are nearly fully satisfied with slight omission of one detail.", "strictMode": false, "evaluationModel": "gpt-4.1", "evaluationCost": 0.002718, "verboseLogs": "Criteria:\nDetermine if the 'actual output' is correct based on the 'expected output'. \n \nEvaluation Steps:\n[\n    \"Compare the actual output to the expected output for exact match.\",\n    \"Check if all required elements or information in the expected output are present in the actual output.\",\n    \"Identify any discrepancies or missing components between the actual and expected outputs.\",\n    \"Determine if the actual output fully satisfies the requirements outlined by the expected output.\"\n] \n \nRubric:\nNone \n \nScore: 0.85"}], "runDuration": 8.736876699957065, "evaluationCost": 0.002718, "order": 0}], "conversationalTestCases": [], "metricsScores": [{"metric": "Correctness [GEval]", "scores": [0.85], "passes": 1, "fails": 0, "errors": 0}], "testPassed": 1, "testFailed": 0, "runDuration": 12.416762500070035, "evaluationCost": 0.002718}}