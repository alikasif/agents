

Parametric memory : LLM weights and biases learned during training. Also know as long term memory
Working memory: Context window. Limited short term memory for most recent conversation .also know as short term memory
RAG: persistent memory used to facilitate recent conversation
Episiodic memory: Researchers are developing episodic memory modules to mimic the human hippocampus, enabling LLMs to form context-specific, short-term memories that can be consolidated into long-term knowledge over time
LangChain memory library: Conversational Memory for LLMs with Langchain | Pinecone
Mem0: mem0ai/mem0: Universal memory layer for AI Agents; Announcing OpenMemory MCP - local and secure memory management.
LangMem: Introduction
How much to remember & What to remember? Memory and State in LLM Applications - Arize AI
LLM memory is fixed in the form of weights & biases.
Cognitive architecture of language agents (CoALA):
Short term:
Working memory () - mostly context window. data structure that perisist across LLM calls. On each LLM call, the LLM input is synthesized from a subset of working memory (e.g., a prompt template and relevant variables). The LLM output is then parsed back into other variables (e.g., an action name and arguments) which are stored back in working memory and used to execute the corresponding action (Figure 3A). Besides the LLM, the working memory also interacts with long-term memories and grounding interfaces. It thus serves as the central hub connecting different components of a language agent.
Long Term
Episodic memory: Episodic memory stores experience from earlier decision cycles. During the planning stage of a decision cycle, these episodes may be retrieved into working memory to support reasoning. An agent can also write new experiences from working to episodic memory as a form of learning
Semantic memory. Semantic memory stores an agent’s knowledge about the world and itself.
Procedural memory: Language agents contain two forms of procedural memory: implicit knowledge stored in the LLM weights, and explicit knowledge written in the agent’s code. The agent’s code can be further divided into two types: procedures that implement actions (reasoning, retrieval, grounding, and learning procedures), and procedures that implement decision-making itself (Section 4.6). During a decision cycle, the LLM can be accessed via reasoning actions, and various code-based procedures can be retrieved and executed. Unlike episodic or semantic memory that may be initially empty or even absent, procedural memory must be initialized by the designer with proper code to bootstrap the agent.
While learning new actions by writing to procedural memory is possible , it is significantly riskier than writing to episodic or semantic memory, as it can easily introduce bugs or allow an agent to subvert its designers’ intentions.
Introduction to AI application memory
Update memory: hot path update & background update
how to set up memory for agents
    brute force by appending in context window
    RAG or fine tuning
    Mem GPT
MemGPT aka Letta: 
letta-ai/letta: Letta is the platform for building stateful agents: open AI with advanced memory that can learn and self-improve over time.
LLMs as Operating Systems: Agent Memory - DeepLearning.AI
isn’t an AI model in itself. It’s a layer that surrounds and enhances a model — one that’s focused entirely on how memory should function in dynamic, agentic systems.
Tiered memory system into a fixed-context LLM Processor, granting it the ability to manage its own memory. By intelligently handling different memory tiers, it extends the context available within the limited context window of the LLM, addressing the issue of constrained context windows common in large language models.
Short-term memory (STM): This is like the model’s current context window. It’s what the model can see and act on right now. But it’s limited — usually just a few thousand tokens.
Long-term memory (LTM): This is external memory — stored as embeddings, indexed by meaning, and capable of holding large volumes of information. Think past conversations, documents, plans, or user behavior.
Memory Manager: This is the brain behind it all — the process that decides what to pull into context, what to store, and how to use memory based on the current task.
The context window lets the model understand what you’re talking about right now — maybe it sees the article draft, your current comments, and recent edits.
The external memory stores older versions of the outline, your previous preferences, tone settings, and some feedback you gave three days ago.
The memory manager looks at your current task and decides: “Ah, they’re revisiting the introduction section. Let’s pull in their original draft and their preferred headline style from last week.”
It’s not pulling everything back in. Just what’s useful — with awareness of timing, task type, and personal context.
Vector embeddings
Structured metadata
Swap logic
OpenAI Memory vs LangMem vs MemGPT vs Mem0 for Long-Term Memory
AI Memory Benchmark: Mem0 vs OpenAI vs LangMem vs MemGPT | Mem0

Actionable Insights
Modular agents: thinking beyond monoliths. Perhaps our most important suggestion is that agents should be structured and modular.

Application memory in Large Language Models (LLMs) and AI applications refers to the mechanisms models use to retain and access information beyond 

their immediate input, crucial for maintaining context, personalization, and accuracy over time. Effectively managing this memory is vital for the 

development of sophisticated AI applications like chatbots and agents.


### Summary of Application Memory

Application memory for LLMs is broadly categorized into **parametric memory** and **working memory**, often supplemented by **persistent memory** 

through techniques like Retrieval-Augmented Generation (RAG) and emerging **episodic memory** modules.


*  **Parametric Memory (Internal Knowledge/Long-Term Memory)**: This refers to the knowledge stored within the LLM's weights and biases, acquired during 

its training phase. This internal knowledge is static and doesn't change unless the model is updated or finetuned.

*  **Working Memory (Short-Term Memory)**: This is primarily the **context window** of the LLM, which holds the most recent conversation or immediately

 relevant information. It's fast to access but has a limited capacity and doesn't persist across tasks or sessions.

*  **Persistent/External Memory (Long-Term Memory)**: This encompasses external data sources that an LLM can access, typically through retrieval mechanisms. 

Unlike internal knowledge, this information can be updated or deleted without retraining the model and can persist across sessions.


### Types of Memory

Beyond the basic categories, the sources detail more specific types of memory, particularly in the context of AI agents:


1. **Internal Knowledge**: As mentioned, this is the inherent knowledge learned by the model from its training data, accessible in all queries.

2. **Short-Term Memory**: The model's context window, used for immediate, context-specific information within a task. Examples include previous messages 

in a conversation.

3. **Long-Term Memory**: External data sources that a model can access via retrieval, such as in a RAG system. This can be structured or unstructured data.

  *  **Episodic Memory**: Stores experiences from earlier decision cycles, similar to the human hippocampus, enabling context-specific short-term memories 

  that can be consolidated. Agents can retrieve these episodes to support reasoning or write new experiences as a form of learning.

  *  **Semantic Memory**: Stores an agent's general knowledge about the world and itself.

  *  **Procedural Memory**: Contains implicit knowledge in LLM weights and explicit knowledge in the agent's code, including procedures for actions 

  (reasoning, retrieval, grounding, learning) and decision-making. Learning new actions by writing to procedural memory is possible but riskier due to potential bugs or subversion of designer intentions.


The choice of memory mechanism depends on the data's frequency of use: essential, frequently used information can be incorporated into internal knowledge via 

finetuning, while rarely needed information resides in long-term memory, and immediate context-specific data uses short-term memory.


### Various Libraries and Solutions

Several libraries and frameworks have emerged to facilitate memory management for LLM applications:


*  **LangChain Memory Library** provides various memory types for conversational LLMs, including:

  *  **ConversationBufferMemory**: Simply appends the entire conversation history to the input prompt, reminding the LLM of past interactions. 

  It ensures no information loss within the context window but leads to slower generation speed and larger token usage.

  *  **ConversationSummaryMemory**: Summarizes the entire conversation history into main points using another LLM, keeping the chat history relatively 

  small and enabling long conversations without exceeding token limits. It requires an additional LLM call for summarization, which can slow down computation.

  *  **ConversationBufferWindowMemory**: Retains only the last 'k' conversations to minimize the context window. It reduces chat history size but only 

  captures recent interactions without compression.

*  **MemGPT (Letta)**: Described as a "layer that surrounds and enhances a model," MemGPT focuses on how memory should function in dynamic, agentic systems. 

It introduces a tiered memory system with a **Memory Manager** that intelligently decides what to pull into the context window and what to store in external

long-term memory based on task, timing, and personal context.

*  **Mem0**: A universal memory layer for AI Agents.

*  **LangMem**: Another library for LLM memory.

*  **Vector Databases**: These are crucial for implementing long-term memory, especially for RAG systems. They store and index information as embeddings, 

allowing for efficient semantic search. Popular libraries and vector databases include:

  *  **FAISS (Facebook AI Similarity Search)**.

  *  **Google’s ScaNN (Scalable Nearest Neighbors)**.

  *  **Spotify’s Annoy (Approximate Nearest Neighbors Oh Yeah)**.

  *  **Hnswlib (Hierarchical Navigable Small World)**.

  *  **Milvus**.

  *  **Weaviate**.

  *  **Pinecone**.

  *  **Qdrant**: Used in the LLM Twin project for storing chunked and embedded documents.


### Issues and Path Forward


**Issues:**

1. **Context Window Limitations**: LLMs have a fixed, limited context length, leading to "information overflow" where older or less relevant parts of a 

conversation are forgotten. This also impacts the ability to handle long documents or complex, multi-turn conversations effectively.

2. **Latency and Cost**: Longer contexts increase processing time and cost, especially for repeated computations in autoregressive decoding. Efficient 

memory management is crucial for reducing latency and cost.

3. **Retrieval Quality**: In RAG systems, ensuring that retrieved documents are relevant and sufficient to answer a query, and avoiding redundant or 

noisy information, is a significant challenge.

4. **Data Structural Integrity**: Text-based models often treat structured data (like tables or queues) as unstructured text when fed into the context, 

making it hard for the model to understand the inherent structure.

5. **Consistency and Hallucinations**: Without memory, LLMs can be inconsistent in their responses. Additionally, relying solely on internal knowledge can 

lead to factual inconsistencies or "hallucinations".


**Path Forward (Solutions and Approaches):**

1. **Retrieval-Augmented Generation (RAG)**: A central solution for providing LLMs with up-to-date, external context from "long-term memory" sources. 

This enhances response quality, reduces hallucinations, and supports personalization.

  *  **Retrieval Optimization**: Strategies include **chunking** documents into manageable pieces, **query rewriting**, **reranking** retrieved results 

  for relevance, **contextual retrieval**, **hybrid search** (combining keyword and embedding search), and **self-querying** to extract metadata for filtering.

  *  **Vector Databases**: Essential for efficiently storing and querying embeddings used in RAG.

2. **Agentic Patterns and Tools**: Agents augment LLMs with abilities to use external tools (e.g., text/image retrievers, SQL executors, web browsers, 

calculators) and engage in complex planning, enabling them to handle more queries and generate higher-quality responses by leveraging diverse resources and maintaining state. Memory is a core component of agents.

3. **Advanced Memory Management Strategies**:

  *  **Summarization**: Summarizing past conversation turns to retain key information without exceeding context limits.

  *  **Overflow to Long-Term Memory**: Storing excess short-term memory information into long-term memory systems.

  *  **Redundancy Removal**: Detecting and removing redundant information to reduce memory footprint.

  *  **Reflection**: Agents can reflect on new information to determine if it should be inserted, merged, or replace existing memory, helping to manage 

  contradictions and keep information up-to-date.

4. **Inference Optimization**: Techniques to make model inference cheaper and faster.

  *  **KV Cache Optimization**: Managing the **key-value (KV) cache** through techniques like **PagedAttention** (dividing cache into non-contiguous blocks),

   **multi-query attention**, **grouped-query attention**, and **cross-layer attention** significantly reduces memory usage and improves throughput.

  *  **Prompt Caching**: Storing and reusing overlapping prompt segments to avoid redundant processing, reducing cost and latency.

  *  **Batching**: Using **continuous batching** processes multiple requests simultaneously, improving throughput and GPU utilization by dynamically 

  adjusting batch sizes.

  *  **Speculative Decoding**: Employs a smaller, faster model to draft tokens, which are then verified by the larger target model, accelerating the

  generation process.

  *  **Parallelism**: Distributing computational loads across hardware using **data parallelism**, **model parallelism** (pipeline and tensor parallelism),

   **context parallelism**, and **sequence parallelism**.

  *  **Kernel Optimization**: Writing specialized code (kernels) like **FlashAttention** for specific hardware architectures to optimize attention

   computation and memory access.

  *  **Decoupling Prefill and Decode**: Running these distinct computational steps of LLM inference on separate machines, as they have different bottlenecks,

   optimizes resource allocation.

5. **Structured Outputs and Grammar**: Using frameworks or APIs that support JSON mode or grammars to ensure models generate outputs in a consistent, 

structured format.





