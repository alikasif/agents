https://towardsdatascience.com/langgraph-scipy-building-an-ai-that-reads-documentation-and-makes-decisions/


https://github.com/ilsilfverskiold/ai-personalized-tech-reports-discord


agents.md file


https://levelup.gitconnected.com/how-to-choose-the-right-embedding-model-for-your-rag-application-44e30876d382


RAG
Embedding free RAG

    https://www.digitalocean.com/community/tutorials/beyond-vector-databases-rag-without-embeddings
    PromptRAG: https://cobusgreyling.medium.com/prompt-rag-vector-embedding-free-retrieval-augmented-generation-c37446b43cdd
    Completion Only RAG: https://www.sensible.so/blog/embeddings-vs-completions-only-rag
    RAG Alternatives: https://raghunaathan.medium.com/exploring-retrieval-augmented-generation-rag-and-its-alternatives-bf9e2f337f88
    Cache augmented generation
    Knowledge-Augmented Generation


Best RAG Tools: https://research.aimultiple.com/retrieval-augmented-generation/

Agentic RAG

RAG 2.0

SOTA Retrieval Algorithms: SPLADE, DRAGON, and Hybrid search. * Contextualizing the Retriever for the Generator: RePlug and In-Context RALM (It uses Frozen RAG and BM25 and then specializes only the retrieval part via reranking.)

    Combined Contextualized Retriever and Generator: k-NN LM and Fusion In Decoder
    SOTA Contextualizaton: REALM and ATLAS

There are three types of RAGs:

    Frozen RAG: We see these all over the industry, they are just POCs.
    SemiFrozen RAG: Here we implement smart Retrievers and try to make them adaptive somehow. We don’t touch the LLM here, only play with retrievers and combine them with the final output.
    Fully trainable RAG: Quite hard to train end-to-end but if done correctly, offers the best performance. Very resource-intensive.


infiniflow/ragflow: RAGFlow is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs


5 Advanced RAG Architectures Beyond Traditional Methods - MachineLearningMastery.com


The evolution of Modern RAG Architectures.


Naive RAG

Contextual Retreival

CAG + RAG

Agentic RAG

	


Naive RAG has emerged almost at the same time as the LLMs have become mainstream with introduction of ChatGPT at the end of 2022. The Retrieval Augmented Generation technique was brought to life in order to solve for issues that native LLMs faced. In short:

    Hallucinations.
    Limited Context window size.
    Lack of access to non-public data.
    Parametric knowledge limited to the latest data the model was trained on.


Advanced techniques to improve Naive RAG.

Some of the more successfully adopted techniques to continuously improve accuracy of Naive RAG systems are the following:

    Query Alteration - there are few techniques that can be employed:
    Query rewriting - ask LLM to rewrite original query to better fit the retrieval process. It can be rewritten in multiple ways. E.g. fixing grammar, simplifying the query to keep short succinct statements.
    Query Expansion - ask LLM to rewrite the query multiple times to create multiple variations of it. Then, run retrieval process multiple times to retrieve more, potentially relevant, context.
    Reranking - rerank the originally retrieved documents using heavier process compared to regular contextual search. Usually, this involves using a larger model and retrieving considerably more documents than needed during the retrieval phase. Reranking also works well with Query Expansion pattern from previous point as it return more data than usual. The overall process is similar to what we are used to seeing in Recommendation Engines.

Fine-Tuning of the embedding model - retrieval of data in some domains (e.g. medical data) works poorly with base embedding models. This is where you might need to fine-tune your own embedding model.


RAG2.0

Contextual Language Models (CLMs) play a central role in the success of RAG 2.0 by fundamentally enhancing how the system integrates and utilizes retrieved information. Unlike traditional language models that rely only on static, pre-trained knowledge, CLMs in RAG 2.0 dynamically incorporate external information both at training and inference time. This dynamic integration allows the models to adapt more effectively to specific domains and tasks, significantly improving accuracy and relevance in retrieval-augmented generation.

Key roles of CLMs in RAG 2.0 include:

    End-to-End Training: CLMs are trained jointly with the retriever, allowing backpropagation of errors through both the retrieval and generation components. This joint optimization leads to better synergy between retrieval and language generation, making the system more robust and aligned to the task-specific retrieval needs.
    Enhanced Context Awareness: By using contextual embeddings generated dynamically based on the current query and conversation history, CLMs enable more nuanced and precise retrieval. This allows RAG 2.0 to fetch information that is highly relevant to the immediate context, improving response quality.
    Improved Faithfulness and Freshness: CLMs learn to stay better grounded in retrieved evidence, reducing hallucinations and increasing the factual accuracy of responses. They are also better at integrating up-to-date or domain-specific knowledge, overcoming limitations of models that rely solely on pre-trained static data.
    Superior Performance: CLMs in RAG 2.0 outperform strong frozen RAG baselines based on large models like GPT-4, showing substantial gains on various benchmarks including open-domain question answering, faithfulness, and freshness across standard datasets.
    Scalability and Efficiency: CLMs handle longer context windows and large knowledge bases more effectively than long-context language models. This makes RAG 2.0 both more scalable and computationally efficient in practical, production-grade scenarios.

Overall, Contextual Language Models are core to why RAG 2.0 works better than traditional RAG systems; they enable seamless integration and optimization of retrieval and generation into a coherent, end-to-end trainable system that provides more accurate, contextually relevant, and scalable results.




First, a Quick Recap: What is Basic "RAG 1.0"?

Traditional RAG (what we can now call "RAG 1.0") is a simple two-step process:

    Retrieve: Given a user query, search a knowledge base (usually a vector database) for the most relevant text chunks or documents.
    Generate: Feed these retrieved chunks and the original query to a Large Language Model (LLM) and ask it to generate an answer based on that context.

The Problem with RAG 1.0: It's often a "single-shot" system. It retrieves once, generates once, and hopes for the best. This leads to common failure modes:

    Retrieving irrelevant information.
    The LLM ignoring the provided context ("context neglect").
    No way to verify the quality of the answer.
    Struggling with complex, multi-step questions.


What is RAG 2.0? The Next-Generation Architecture

RAG 2.0 isn't a single, rigid specification but rather a family of advanced architectures that make the RAG process more iterative, adaptive, and intelligent. The core philosophy is moving from a simple "retrieve-then-read" pipeline to a more sophisticated "retrieve-read-retrieve-again" loop with various quality checks and optimizations.

Think of it as upgrading from a simple Google search to a conversation with a expert research assistant who looks up information, thinks about it, asks clarifying questions, and then looks up more information to fill in the gaps.
Key Pillars and Components of RAG 2.0 Architecture

Here are the defining characteristics and techniques that make up RAG 2.0:
1. Advanced Retrieval Strategies

Instead of just a single vector similarity search, RAG 2.0 employs a multi-faceted approach:

    Hybrid Search: Combines dense vector search (for semantic meaning) with sparse lexical search (like BM25 for keyword matching) for better recall.
    Multi-Query & Query Transformation: The system automatically rephrases or decomposes the user's original query into multiple, simpler queries to retrieve more diverse and comprehensive context.
    Example: For "Compare the marketing strategies of Apple and Samsung," the system might generate separate queries for "Apple marketing strategy" and "Samsung marketing strategy."
    Step-Back Prompting / Query Expansion: The LLM is prompted to generate a more fundamental, conceptual question behind the user's query to retrieve foundational knowledge.
    Metadata Filtering: Heavily uses metadata (e.g., date, author, document type) to filter results before or after the semantic search, ensuring relevance and recency.

2. Iterative "Retrieve-and-Read" Loops

This is the heart of RAG 2.0. The system doesn't just retrieve once.

    Self-RAG / Adaptive RAG: The LLM acts as a judge. After an initial retrieval and generation attempt, it critiques its own work.
    "Is the retrieved context relevant?"
    "Is my answer supported by the context?"
    "Is the answer sufficient, or do I need more information?"
    Based on this self-critique, the system can decide to trigger a new, more refined retrieval or proceed with final answer generation.
    Corrective RAG (CRAG): A lightweight "correcter" model assesses the quality of the retrieved documents. If they are deemed unreliable, it can kick off a web search or a different retrieval strategy to "correct" the knowledge gap.

3. Fine-Tuning and Specialization

RAG 2.0 systems are often optimized end-to-end.

    Fine-tuning the Embedding Model: The model that creates the vector representations is fine-tuned on domain-specific data, making retrieval much more accurate for specialized topics (e.g., legal, medical).
    Fine-tuning the LLM: The generator LLM is fine-tuned to better follow instructions specific to the RAG task, such as "strictly base your answer on the provided context."

4. Agentic and Reasoning Capabilities

RAG 2.0 often incorporates principles of AI Agents.

    Tool Use: The RAG system can use other tools beyond its vector database. For example, if the retrieved information is outdated, it can call a web search API to get the latest data.
    Decomposition: For complex questions, an agent can break the question down into sub-questions, run a RAG cycle for each, and then synthesize a final answer.

A Simple RAG 2.0 Workflow Example

Let's see how these components come together for the query: "What were the main causes of the 2008 financial crisis, and what similar regulations exist today to prevent a recurrence?"

    Query Transformation:

    The system decomposes this into: 1) "Main causes of 2008 financial crisis," 2) "Financial regulations enacted after 2008," 3) "Current financial regulations in place."

    Hybrid Retrieval:

    A hybrid search is performed for each sub-query against a database of financial documents and news articles, filtered for high authority and post-2008 dates where relevant.

    Iterative Retrieval & Self-Critique:

    The LLM generates a draft answer for "current regulations." It self-critiques: *"My context only has regulations up to 2015 (Dodd-Frank). This is insufficient."*
    This critique triggers a new retrieval, perhaps with a tool call to a web search for "financial regulations 2023-2024."

    Synthesis and Final Answer:

    With comprehensive, multi-perspective context from all retrieval steps, the LLM synthesizes a final, well-grounded answer that addresses both parts of the complex original question.


Summary: RAG 1.0 vs. RAG 2.0


FeatureRAG 1.0 (Naive/Basic)RAG 2.0 (Advanced)RetrievalSingle, simple vector searchHybrid, multi-query, adaptiveProcessLinear ("Retrieve-then-Generate")Iterative, recursive loopIntelligenceDumb retriever, smart generatorSmart retriever, smart generator, self-criticQuery HandlingHandles the query as-isTransforms, decomposes, and expands the queryGoalProvide context to the LLMGuarantee the answer is correct and grounded

In essence, RAG 2.0 is a move from a static pipeline to a dynamic, self-correcting, and highly reliable knowledge system. It's the necessary evolution to make RAG production-ready for critical enterprise applications where accuracy is paramount.


What Makes RAG 2.0 Different?

The fundamental shift in RAG 2.0 is from a naive, linear pipeline to an intelligent, adaptive system. The core advancements can be summarized as follows:

    End-to-End Optimization: Unlike RAG 1.0, which uses "frozen" off-the-shelf models for retrieval and generation, RAG 2.0 advocates for pre-training, fine-tuning, and aligning all components as a single, integrated system. This end-to-end optimization, often backpropagating through both the retriever and the generator, maximizes performance for specific tasks.
    Intelligent and Iterative Retrieval: RAG 2.0 systems don't just retrieve once. They use techniques like multi-hop retrieval (sequential searches to gather information step-by-step) and self-critique (evaluating the quality of retrieved documents) to fetch better context, even triggering web searches if initial results are poor.
    Advanced Query Understanding: The system actively rewrites and refines user queries before retrieval. This involves expanding acronyms, adding missing entities, and breaking down complex questions into simpler sub-questions to drastically improve retrieval accuracy.
    Sophisticated Data Indexing: Instead of one embedding per document, RAG 2.0 often uses multi-vector indexing, creating separate embeddings for different parts of a document (like the title, summary, and key entities) to capture more nuanced semantic meaning.
    Hybrid Search as a Standard: RAG 2.0 relies on combining multiple search techniques—dense vector search (for semantic meaning), sparse keyword search (like BM25 for exact matches), and sometimes even knowledge graph traversal—to ensure both high recall and high precision.

🏗️ Architectural Patterns & Components

A RAG 2.0 system features a more complex and orchestrated flow. Here are some of the key advanced architectural patterns:


PatternCore Concept & WorkflowIdeal Use CaseSelf-RAGSelf-critique & iterative retrieval. Dynamically critiques its own retrieved context and generation, issuing new retrieval queries to fill information gaps during the generation process.Exploratory research, long-form content creation where information needs evolve.Corrective RAG (CRAG)Self-grading & correction. "Grades" retrieved documents for relevance; if they fail a confidence threshold, it automatically triggers corrective actions like a web search.High-stakes applications requiring maximal factual accuracy (legal, medical, financial).Agentic RAGMulti-agent orchestration. Employs a "meta-agent" to coordinate multiple "document agents," each responsible for a specific document or data source, for complex synthesis.Automated research, executive decision support requiring synthesis from multiple specialized sources.Adaptive RAGDynamic strategy selection. Analyzes query complexity and dynamically chooses the best retrieval strategy (e.g., single-source vs. multi-source) for speed and accuracy.Enterprise search systems with highly variable query types.

A typical high-level RAG 2.0 data pipeline integrates these smart components:

    Query Understanding Layer: Rewrites and refines the user's original query.
    Retriever Orchestration Engine: Routes the refined query to the appropriate retrievers (vector, keyword, graph).
    Indexing Layer: Houses a Multi-Vector Database and potentially a Knowledge Graph.
    Context Optimizer: Reranks and summarizes retrieved chunks to fit context windows and maximize relevance.
    LLM Prompt Composer & Inference: Constructs the final prompt and generates the answer.

⚙️ Implementation & Security Focus

Building a production-grade RAG 2.0 system requires attention to several key areas:

    Database Requirements: You need a powerful database that natively supports hybrid search (vector + keyword) and can handle metadata filtering at scale. Performance on massive datasets is critical.
    The Shift from Orchestration to Integration: RAG 2.0's tightly coupled, iterative stages are less suited to simple orchestration frameworks. The focus shifts to deep integration and optimization of each stage of the pipeline.
    Security in Multi-Tenant Environments (RAG 2.0 Security): A major advancement is the focus on runtime security. This involves session-based policy evaluation and strict metadata filtering in the vector database to prevent data leaks, ensuring User A cannot retrieve documents from User B based on semantic similarity alone.

🚀 Getting Started & Tools

Moving from a basic RAG 1.0 prototype to a RAG 2.0 system involves a mindset shift from simple orchestration to building an end-to-end, optimized search system.

You can start by incrementally adding advanced components to a basic RAG pipeline, such as a query rewriter or a hybrid retriever. The open-source project RAGFlow is cited as an example of a platform built with these RAG 2.0 pipeline principles in mind.

Types of RAG


Naive RAG: The Classic Approach


Retrieve and Rerank RAG

This architecture improves on the naive approach by introducing a second step: reranking.

Workflow:

    Initial retrieval is broad, returning many candidate documents (e.g., top-50).
    A reranking model (such as BERT-based cross-encoder) evaluates these documents more deeply for relevance.
    The top-n documents after reranking are selected.
    These top-n documents form the context for the LLM


Multimodal RAG

This architecture expands RAG’s capabilities beyond just text data, allowing it to process images, videos, audio, and more.


Graph RAG

Graph RAG introduces graph databases (like Neo4j) to model relationships between documents and concepts.

Workflow:

    Chunks of documents are stored as nodes.
    Explicit relationships (e.g., citations, authorship, topics) form edges.
    Query traversal finds relevant nodes based on both semantic similarity and graph structure.
    Retrieved subgraphs form the context for the LLM.


Hybrid RAG: Vector DB + Graph DB

Hybrid RAG combines the strengths of semantic vector retrieval and structured graph traversal.

Workflow:

    A semantic search retrieves contextually similar chunks.
    Simultaneously, a graph database retrieves related documents via explicit relationships.
    The results from both systems are merged, deduplicated, and prioritized.
    The hybrid context is passed to the LLM.


Agentic RAG with Router Agent

This architecture uses a single intelligent agent to dynamically decide how to handle each query.

Workflow:

    A Router Agent interprets the user query.
    Based on the query type and intent, it selects the best retrieval path (e.g., which database or modality).
    The Router Agent orchestrates the retrieval and constructs the final prompt.
    LLM generates a context-aware response.


 Multi-Agent RAG

This sophisticated architecture leverages multiple specialized agents that collaborate like a team.

Workflow:

    A Master Agent receives the query and delegates sub-tasks.
    Specialized agents handle domain-specific retrieval, data transformation, summarization, or personalization.
    Agents interact to refine and align context.
    The final context is compiled and passed to the LLM.


HyDE (Hypothetical Document Embeddings)

- Queries are not semantically similar to documents.

- This technique generates a hypothetical answer document from the query before retrieval.

- Uses this generated document’s embedding to find more relevant real documents.


 Corrective RAG

- Validates retrieved results by comparing them against trusted sources (e.g., web search).

- Ensures up-to-date and accurate information, filtering or correcting retrieved content before passing to the LLM.


Adaptive RAG

- Dynamically decides if a query requires a simple direct retrieval or a multi-step reasoning chain.

- Breaks complex queries into smaller sub-queries for better coverage and accuracy


Agentic RAG: Merging RAG with AI Agents

Agentic RAG refers to a more intelligent and dynamic Retrieval-Augmented Generation system where an “agent” plays a key role in orchestrating processes. The agent intelligently determines which resources or databases are most relevant for a user’s query, making it capable of handling more complex, multi-tasking scenarios. It is an evolution from traditional RAG systems, offering greater adaptability and decision-making by incorporating additional logic or heuristics into the retrieval and response generation pipeline.

    Agentic: The system works on its own, making decisions and taking actions depending on the situation.
    RAG (Retrieval-Augmented Generation): It mixes information from a knowledge base with the AI’s ability to create responses.


Query Planning Agentic RAG

Query Planning Agentic RAG (Retrieval-Augmented Generation) is a methodology designed to handle complex queries efficiently by leveraging multiple parallelizable subqueries across diverse data sources. This approach combines intelligent query division, distributed processing, and response synthesis to deliver accurate and comprehensive results.


Adaptive RAG 

Adaptive RAG dynamically chooses between different strategies for answering questions—ranging from simple single-step approaches to more complex multi-step or even no-retrieval processes—based on the complexity of the query. This selection is facilitated by a classifier, which analyzes the query’s nature and determines the optimal approach.


Agentic Corrective RAG


Corrective RAG (CRAG)

CRAG aims to address the above issues by introducing mechanisms to self-correct retrieval results, enhancing document utilization, and improving generation quality.


Agentic Corrective RAG System Workflow

The idea is to couple a RAG system with a few checks in place and perform web searches if there is a lack of relevant context documents to the given user query 


Self-Reflective RAG

Self-reflective RAG (Retrieval-Augmented Generation) is an advanced approach in natural language processing (NLP) that combines the capabilities of retrieval-based methods with generative models while adding an additional layer of self-reflection and logical reasoning. For instance, self-reflective RAG helps in retrieval, re-writing questions, discarding irrelevant or hallucinated documents and re-try retrieval. In short, it was introduced to capture the idea of using an LLM to self-correct poor-quality retrieval and/or generations.


Speculative RAG 

Speculative RAG is a smart framework designed to make large language models (LLMs) both faster and more accurate when answering questions. It does this by splitting the work between two kinds of language models:

    A small, specialized model that drafts potential answers quickly.
    A large, general-purpose model that double-checks these drafts and picks the best one.


Self Route Agentic RAG 

Self Route is a design pattern in Agentic RAG systems where Large Language Models (LLMs) play an active role in deciding how a query should be processed. The approach relies on the LLM’s ability to self-reflect and determine whether it can generate an accurate response based on the context provided. If the model decides it cannot generate a reliable response, it routes the query to an alternative method, such as a long-context model, for further processing. This architecture leverages the LLM’s internal calibration for determining answerability to optimize performance and cost. Introduced in Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach, this method combines Retrieval-Augmented Generation (RAG) and Long Context (LC) to achieve cost efficiency while maintaining performance comparable to LC. Self Route utilizes the LLM itself to route queries through self-reflection, operating on the assumption that LLMs are well-calibrated in predicting whether a query is answerable given the provided context. 



RAG vs. Long-context LLMs

Language models have come a long way, starting from the good old ChatGPT with a context window as small as a few thousand to Gemini 1.5 Pro handling up to a million tokens at once. In search of ways to process more information than the initial context windows could handle, scientists developed retrieval augmented generation (RAG) – which would be tied up to their language models and help retrieve real-time and accurate answers from external documents. But now that we have around 1000 times bigger context windows that can handle a whole encyclopedia, a question naturally arises: Do we still need RAG?


Order Preserving (OP)-RAG: Revisiting RAG in long-context generation

OP-RAG is a technique that improves the quality of RAG in long-context question-answer applications. With OP-RAG, as the number of retrieved chunks increases, the answer quality initially rises, and then declines, forming an inverted U-shaped curve.

RAG vs. Long-context LLMs | SuperAnnotate


How OP-RAG works

Let's dive into how OP-RAG works. Imagine you have a long document, referred to as 'd'. It is then cut into several sections or 'chunks', making sure each one is sliced in a consistent, sequential order. We label these chunks as c1, c2, and so on, right up to cn, where 'n' represents the total number of chunks.

When someone submits a query, like 'q', we need to identify which chunks are most relevant. This is done by using cosine similarity, which measures how closely the content of each chunk relates to the query. This gives us a score, si, for each chunk, indicating its relevance to the query.

The next step is where OP-RAG stands out. The top chunks are picked based on their relevance scores, but instead of rearranging them by these scores, we keep them in the order they appear in the document. This means if chunk c5 is originally before c7 in the document, it will stay that way in our lineup, regardless of their individual relevance scores.

This method differs from traditional RAG, where chunks are ordered solely by relevance, possibly disrupting the natural flow of information. By keeping the original sequence, OP-RAG helps maintain the logical progression of the text, which is essential for producing coherent and accurate responses.



Despite the impressive capabilities of long-context LLMs, there are inherent limitations to processing extremely large amounts of text in a single pass. As the context window expands, the model’s ability to focus on relevant information can diminish, potentially leading to a degradation in answer quality. This phenomenon highlights a critical trade-off between the breadth of available information and the precision of the model’s output.



OP-RAG is a technique that improves the quality of RAG in long-context question-answer applications. With OP-RAG, as the number of retrieved chunks increases, the answer quality initially rises, and then declines, forming an inverted U-shaped curve.

At certain points, OP-RAG achieves higher quality results with much less tokens than long-context LLMs. Experiments on public benchmarks showed the superiority of OP-RAG, which we'll examine soon.


Long COntext RAG Perf

The performance analysis of Retrieval Augmented Generation (RAG) when utilizing long context lengths reveals that longer context models and RAG are synergistic, enabling RAG systems to effectively include more relevant documents. However, this improvement is not uniform across all models and context sizes, and many models exhibit significant performance limitations as context length increases.

The study, which ran over 2,000 experiments on 13 LLMs, found the following key outcomes regarding long context RAG performance:
1. Benefits of Increased Context Length

    Increased Retrieval Success: Retrieving more documents increases the likelihood that the right information is passed to the Large Language Model (LLM). Modern LLMs with long context lengths can take advantage of this to improve the overall RAG system quality.
    Recall Saturation: Experiments showed that the retrieval recall score (the upper bound on generation performance) increased significantly with context length. For datasets like Databricks DocsQA, HotPotQA, and FinanceBench, retrieval recall did not saturate until context lengths reached 96k or 128k tokens. This demonstrates that increased context size offers the promise of capturing additional information to increase system quality.
    Initial Performance Boost: There was an observable increase in performance across all models when context length increased from 2k to 4k tokens, and this increase persisted for many models up to 16k to 32k context length.

2. Limitations and Performance Saturation

While longer context provides benefits, the study found that using longer context does not uniformly increase RAG performance.

    Saturation Point: For most models, performance decreases after a certain saturation point. Examples include:
    GPT-4-turbo and Claude-3-sonnet starting to decrease performance after 16k tokens.
    Mixtral-instruct performance decreasing after 4k tokens.
    DBRX-instruct performance decreasing after 8k tokens.
    Llama-3.1-405b performance starting to decrease after 32k tokens.
    GPT-4-0125-preview starting to decrease after 64k tokens.
    Effective Context Length: The effective context length—the amount of usable context length beyond which model performance begins to decrease—can be much shorter than the maximum context length claimed by the model developers.
    Top Performers: A few recent models showed improved long context behavior with little to no performance deterioration as context length increased. These models include gpt-4o, claude-3.5-sonnet, and gpt-4o-mini. The gpt-4o-2024-05-13 model, for instance, had the highest overall average performance across all context lengths tested (0.709).
    Developer Consideration: Developers must be mindful in selecting the optimal number of documents (and thus context size), as this choice likely depends on both the generation model being used and the specific task at hand.

3. Distinct Failure Modes at Long Context

Models fail in highly distinct ways, particularly noticeable after certain context lengths. These behaviors suggest a lack of sufficient long context post-training.

Model CategoryExample ModelPrimary Failure ModeDetails/ObservationsCommercialGPT-4Providing the wrong answerAlso occasionally produces irrelevant or random content.CommercialClaude-3-sonnetFailing to follow instructions (often refusing due to copyright concerns)The refusal rate due to copyright increased drastically with context length (e.g., from 3.7% at 16k tokens to 49.5% at 64k tokens).Open SourceLlama-3.1-405b-instructProviding the wrong answerRarely generated repeated or random content, showing good instruction following performance, similar to GPT-4.Open SourceMixtral-instructOutputting repeated content or random content (e.g., repeating the Chinese word for "dream")

Open SourceDBRX-instructFailing to follow instructions by summarizing the context instead of answering the questionThe failure to follow instructions for DBRX increased sharply with context length (e.g., from 5.2% at 8k to 50.4% at 32k).


